import streamlit as st
from dotenv import load_dotenv
from PyPDF2 import PdfReader
import pdfplumber
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.chains import ConversationalRetrievalChain
from langchain.prompts import PromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain.schema import messages_from_dict
import os
import cv2
import numpy as np
from PIL import Image
import pdf2image
import pandas as pd
from paddleocr import PaddleOCR
from htmlTemplates1 import css, bot_template, user_template
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
# Kh·ªüi t·∫°o PaddleOCR (ch·ªâ 1 l·∫ßn)
@st.cache_resource
def init_paddle_ocr():
    """Kh·ªüi t·∫°o PaddleOCR v·ªõi cache ƒë·ªÉ tr√°nh reload nhi·ªÅu l·∫ßn"""
    try:
        ocr = PaddleOCR(use_angle_cls=True, lang='en', use_gpu=False)
        return ocr
    except Exception as e:
        st.error(f"L·ªói kh·ªüi t·∫°o PaddleOCR: {str(e)}")
        return None

def detect_scan_pdf(pdf_path):
    """Ki·ªÉm tra PDF c√≥ ph·∫£i l√† scan kh√¥ng b·∫±ng c√°ch ph√¢n t√≠ch n·ªôi dung text"""
    try:
        with pdfplumber.open(pdf_path) as pdf:
            total_chars = 0
            for page in pdf.pages[:3]:  # Ki·ªÉm tra 3 trang ƒë·∫ßu
                text = page.extract_text()
                if text:
                    total_chars += len(text.strip())
            
            # N·∫øu √≠t h∆°n 100 k√Ω t·ª± trong 3 trang ƒë·∫ßu => c√≥ th·ªÉ l√† scan
            return total_chars < 100
    except:
        return False

def extract_tables_from_text_pdf(pdf_path):
    """Tr√≠ch xu·∫•t b·∫£ng t·ª´ PDF text b·∫±ng pdfplumber"""
    tables_text = ""
    try:
        with pdfplumber.open(pdf_path) as pdf:
            for page_num, page in enumerate(pdf.pages):
                tables = page.extract_tables()
                if tables:
                    tables_text += f"\n--- Table of page {page_num + 1} ---\n"
                    for table_num, table in enumerate(tables):
                        if table:
                            # Chuy·ªÉn table th√†nh DataFrame ƒë·ªÉ format ƒë·∫πp
                            df = pd.DataFrame(table[1:], columns=table[0] if table[0] else None)
                            tables_text += f"\nTable {table_num + 1}:\n"
                            tables_text += df.to_string(index=False) + "\n\n"
    except Exception as e:
        st.warning(f"L·ªói tr√≠ch xu·∫•t b·∫£ng t·ª´ PDF text: {str(e)}")
    
    return tables_text

def extract_tables_from_scan_pdf(pdf_path, ocr):
    """Tr√≠ch xu·∫•t b·∫£ng t·ª´ PDF scan b·∫±ng PaddleOCR"""
    tables_text = ""# t·∫°o chu·ªói tables_text
    try:
        # Chuy·ªÉn PDF th√†nh ·∫£nh
        images = pdf2image.convert_from_path(pdf_path, 
                                             dpi=200,# ƒë·ªô ph√¢n gi·∫£i 200200
                                             poppler_path=r"C:/Release-24.08.0-0/poppler-24.08.0/Library/bin"
                                             )
        
        for page_num, image in enumerate(images):# duy·ªát t·ª´ng ·∫£nh. page)num l√† s·ªë th·ª© t·ª± trang
            # Chuy·ªÉn PIL image th√†nh numpy array
            img_array = np.array(image) # chuy·ªÉn ·∫£nh sang m·∫£ng numpy ƒë·∫ªe x·ª© l√≠ b·∫±ng opencv
            img_cv = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)# numpy-> RGBRGB
            
            # S·ª≠ d·ª•ng OCR ƒë·ªÉ detect text v√† box
            result = ocr.ocr(img_cv, cls=True) #g·ªçi h√†m OCR ƒë·ªÉ nh·∫≠n di·ªán ch·ªØ
            
            if result and result[0]:# n·∫øu c√≥ kq tr·∫£ v·ªÅ t·ª´ oCR th√¨ ti·∫øp t·ª•c x·ª≠ l√≠
                # Ph√¢n t√≠ch layout ƒë·ªÉ t√¨m b·∫£ng
                tables_text += f"\n--- Content of page scan {page_num + 1} ---\n"
                
                # S·∫Øp x·∫øp text theo v·ªã tr√≠ (top-to-bottom, left-to-right)
                text_boxes = [] # kh·ªüi t·∫°o danh s√°ch text box ƒë·ªÉ luu∆∞ t·ªça ƒë·ªô c√°c kh·ªëi VB
                for line in result[0]:# duy·ªát t·ª´ng d√≤ng vƒÉn b·∫£n trong ·∫£nh·∫£nh
                    if line:# n·∫øu d√≤ng k r·ªóng
                        box = line[0] # danh s√°ch 4 ƒëi·ªÉm t·ªça ƒë·ªô c·ªßa khung ch·ªØch·ªØ
                        text = line[1][0] # n·ªôi dung vbvb
                        confidence = line[1][1] # ƒë·ªô tin c·∫≠y 0-11
                        
                        if confidence > 0.5:  # Ch·ªâ l·∫•y text c√≥ ƒë·ªô tin c·∫≠y cao
                            # T√≠nh to√°n v·ªã tr√≠ trung t√¢m
                            center_y = (box[0][1] + box[2][1]) / 2
                            center_x = (box[0][0] + box[2][0]) / 2
                            text_boxes.append((center_y, center_x, text))
                
                # S·∫Øp x·∫øp theo v·ªã tr√≠
                text_boxes.sort(key=lambda x: (x[0], x[1]))# sx tr√™n xu·ªëng , tr√°i -ph·∫£iph·∫£i
                
                # Gh√©p text th√†nh ƒëo·∫°n vƒÉn
                current_line_y = None
                current_line_texts = [] # danh s√°ch c√°c t·ª´ trong d√≤ngd√≤ng
                
                for y, x, text in text_boxes:# l·∫∑p qua t·ª´ng ch·ªØ OCR trong ds text_boxbox
                    if current_line_y is None or abs(y - current_line_y) > 20:
                        # D√≤ng m·ªõi
                        if current_line_texts: # n·∫øu ƒëang c√≥ nd d√≤ng c≈©c≈©
                            tables_text += " ".join(current_line_texts) + "\n" # ghi v√†o table texttext
                        current_line_texts = [text]# h·∫øt nd d√≤ng c≈© th√¨ 
                        current_line_y = y #kh·ªüi t·∫°o d√≤ng m·ªõi v·ªõi t·ª´ hien taitai
                    else:
                        # C√πng d√≤ng
                        current_line_texts.append(text)
                
                # Th√™m d√≤ng cu·ªëi
                if current_line_texts:
                    tables_text += " ".join(current_line_texts) + "\n"
                    
    except Exception as e:
        st.warning(f"L·ªói OCR PDF scan: {str(e)}")
    
    return tables_text

def get_pdf_text_enhanced(pdf_docs):
    """Tr√≠ch xu·∫•t text t·ª´ PDF v·ªõi h·ªó tr·ª£ OCR v√† table extraction"""
    if not pdf_docs:
        return ""
    #khoi tao OCR v√† giao di·ªándi·ªán
    ocr = init_paddle_ocr() #kh·ªüi t·∫°o m√¥ h√¨nh paddle OCR
    text = ""
    progress_bar = st.progress(0) #d√πng ƒë·ªÉ hi·ªÉn th·ªã ti·∫øn ƒë·ªô tr√™n streamlitstreamlit
    status_text = st.empty()# d√πng ƒë·ªÉ h th·ªã tr·∫°ng th√°ith√°i
    
    for i, pdf_file in enumerate(pdf_docs):#duy·ªát qua t·ª´ng filefile
        try:
            status_text.text(f'ƒêang ph√¢n t√≠ch {pdf_file.name}...')
            
            # L∆∞u file t·∫°m ƒë·ªÉ x·ª≠ l√Ω
            temp_path = f"temp_{pdf_file.name}"
            with open(temp_path, "wb") as f:
                f.write(pdf_file.getvalue())
            
            # Ki·ªÉm tra lo·∫°i PDF
            is_scan = detect_scan_pdf(temp_path)
            
            if is_scan:# n·∫øu l√† PDF scan d√πng OCROCR
                status_text.text(f'üîç PDF scan detected - ƒêang OCR {pdf_file.name}...')
                
                # X·ª≠ l√Ω PDF scan
                if ocr:
                    scan_text = extract_tables_from_scan_pdf(temp_path, ocr)
                    if scan_text.strip():
                        text += f"\n=== PDF SCAN: {pdf_file.name} ===\n"# th√™m tieu ƒë·ªÅ ƒë·ªÉ ƒëanhs gi·∫•u PDF scan
                        text += scan_text + "\n" # n·ªëi scan_text v√†o text chung 
                    else:
                        st.warning(f"Kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c text t·ª´ PDF scan: {pdf_file.name}")
                else:
                    st.error("PaddleOCR kh√¥ng kh·∫£ d·ª•ng!")
                    
            else:
                status_text.text(f'üìÑ PDF text detected - ƒêang x·ª≠ l√Ω {pdf_file.name}...')
                
                # X·ª≠ l√Ω PDF text th√¥ng th∆∞·ªùng
                pdf_reader = PdfReader(temp_path)# ƒë·ªçc file PDF d·∫°ng text
                regular_text = ""# kh·ªüi t·∫°o chu·ªói r·ªóng 
                
                for page_num, page in enumerate(pdf_reader.pages):#l·∫∑p qua t·ª´ng trang c·ªßa pdf 
                    page_text = page.extract_text() #trichs xu·∫•t text t·ª´ c√°c trangtrang
                    if page_text:#n·∫øu page_text k r·ªóng
                        regular_text += f"\n--- Page {page_num + 1} ---\n"# th√™m v√†o regular text v√†o 1 ƒëo·∫°n ch√∫ th√≠ch ph√¢n trang
                        regular_text += page_text + "\n" # n·ªëi page text v√†o regular_texttext
                
                # Tr√≠ch xu·∫•t b·∫£ng t·ª´ PDF text
                tables_text = extract_tables_from_text_pdf(temp_path)
                
                if regular_text.strip() or tables_text.strip():#ki·ªÉm tra xem c√≥ nd n√†o dc tr√≠ch ra kk
                    text += f"\n=== PDF TEXT: {pdf_file.name} ===\n"#th√™m ti√™u ƒë·ªÅ
                    text += regular_text + tables_text + "\n"# th√™m v√†o bi·∫øn text t·ªïng 
            
            # X√≥a file t·∫°m
            if os.path.exists(temp_path):
                os.remove(temp_path)
                
        except Exception as e:
            st.error(f"‚ùå L·ªói x·ª≠ l√Ω {pdf_file.name}: {str(e)}")
            continue
            
        progress_bar.progress((i + 1) / len(pdf_docs))
    
    progress_bar.empty()
    status_text.empty()
    
    if not text.strip():
        st.warning("‚ö†Ô∏è Kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c text t·ª´ b·∫•t k·ª≥ PDF n√†o!")
    else:
        st.success(f"‚úÖ ƒê√£ x·ª≠ l√Ω {len(pdf_docs)} file PDF th√†nh c√¥ng!")
        
        # Hi·ªÉn th·ªã th·ªëng k√™
        word_count = len(text.split())
        char_count = len(text)
        st.info(f"üìä Th·ªëng k√™: {word_count:,} t·ª´, {char_count:,} k√Ω t·ª±")
    
    return text

def get_text_chunks(text):
    """Chia text th√†nh c√°c chunk nh·ªè"""
    if not text or not text.strip():
        st.warning("Text r·ªóng, kh√¥ng th·ªÉ t·∫°o chunks!")
        return []
    
    text_splitter = RecursiveCharacterTextSplitter(
        #separators=["\n\n", "\n", ".", "!", "?", " "],
        separators=["\n\n", "\n", " "],
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len
    )
    
    chunks = text_splitter.split_text(text)
    
    if not chunks:
        st.warning("Kh√¥ng th·ªÉ chia text th√†nh chunks!")
        return []
    
    st.info(f"üìù ƒê√£ t·∫°o {len(chunks)} text chunks")
    return chunks

def get_vectorstore(text_chunks, api_key):
    """T·∫°o vector store t·ª´ text chunks"""
    if not text_chunks:
        st.error("Kh√¥ng c√≥ text chunks ƒë·ªÉ t·∫°o vector store!")
        return None
    
    try:
        embeddings = GoogleGenerativeAIEmbeddings(
            model="models/embedding-001", 
            google_api_key=api_key
        )
        vectorstore = FAISS.from_texts(texts=text_chunks, embedding=embeddings)
        st.success("‚úÖ ƒê√£ t·∫°o vector store th√†nh c√¥ng!")
        return vectorstore
    except Exception as e:
        st.error(f"‚ùå L·ªói t·∫°o vector store: {str(e)}")
        return None

def get_conversation_chain(vectorstore, api_key):
    """T·∫°o conversation chain"""
    if not vectorstore:
        return None
    
    try:
        # prompt_template = """
        # H√£y tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch chi ti·∫øt d·ª±a tr√™n n·ªôi dung ƒë∆∞·ª£c cung c·∫•p. 
        # N·∫øu th√¥ng tin c√≥ b·∫£ng ho·∫∑c d·ªØ li·ªáu c√≥ c·∫•u tr√∫c, h√£y tr√¨nh b√†y r√µ r√†ng.
        # N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin trong n·ªôi dung, h√£y n√≥i "Th√¥ng tin kh√¥ng c√≥ trong t√†i li·ªáu".
        
        # N·ªôi dung: {context}
        
        # C√¢u h·ªèi: {question}
        
        # Tr·∫£ l·ªùi:
        # """
        prompt_template = """
        Answer the question as detailed as possible from the provided context, make sure to provide all the details, if the answer is not in
        provided context just say, "answer is not available in the context", don't provide the wrong answer
        Context:\n {context}?\n
        Question: \n{question}\n

        Answer:
        """
        prompt = PromptTemplate(
            template=prompt_template, 
            input_variables=["context", "question"]
        )

        llm = ChatGoogleGenerativeAI(
            model="gemini-1.5-pro-latest", 
            temperature=0.3, 
            google_api_key=api_key
        )

        conversation_chain = ConversationalRetrievalChain.from_llm(
            llm=llm,
            retriever=vectorstore.as_retriever(search_kwargs={"k": 5}),
            combine_docs_chain_kwargs={"prompt": prompt}
        )
        
        return conversation_chain
    except Exception as e:
        st.error(f"‚ùå L·ªói t·∫°o conversation chain: {str(e)}")
        return None

def handle_userinput_with_manual_history(user_question):
    """X·ª≠ l√Ω input c·ªßa user v√† l∆∞u l·ªãch s·ª≠ chat"""
    if "chat_history_dict" not in st.session_state:
        st.session_state.chat_history_dict = []

    # Th√™m c√¢u h·ªèi m·ªõi
    st.session_state.chat_history_dict.append({
        "type": "human", 
        "data": {"content": user_question}
    })

    try:
        # Chuy·ªÉn l·ªãch s·ª≠ dict th√†nh list message object
        chat_history_messages = messages_from_dict(st.session_state.chat_history_dict)

        # G·ªçi chain
        response = st.session_state.conversation.invoke({
            "question": user_question,
            "chat_history": chat_history_messages
        })

        answer = response.get("answer", "Xin l·ªói, t√¥i kh√¥ng th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y.")

        # Th√™m c√¢u tr·∫£ l·ªùi v√†o l·ªãch s·ª≠
        st.session_state.chat_history_dict.append({
            "type": "ai", 
            "data": {"content": answer}
        })

    except Exception as e:
        st.error(f"‚ùå L·ªói x·ª≠ l√Ω c√¢u h·ªèi: {str(e)}")
        st.session_state.chat_history_dict.append({
            "type": "ai", 
            "data": {"content": "Xin l·ªói, ƒë√£ c√≥ l·ªói x·∫£y ra khi x·ª≠ l√Ω c√¢u h·ªèi c·ªßa b·∫°n."}
        })

    # Hi·ªÉn th·ªã to√†n b·ªô l·ªãch s·ª≠
    for msg in st.session_state.chat_history_dict:
        if msg["type"] == "human":
            st.write(user_template.replace("{{MSG}}", msg["data"]["content"]), 
                    unsafe_allow_html=True)
        else:
            st.write(bot_template.replace("{{MSG}}", msg["data"]["content"]), 
                    unsafe_allow_html=True)

def main():
    """H√†m ch√≠nh c·ªßa ·ª©ng d·ª•ng"""
    load_dotenv()
    api_key = os.getenv("GOOGLE_API_KEY")
    
    # Ki·ªÉm tra API key
    if not api_key:
        st.error("üîë Kh√¥ng t√¨m th·∫•y GOOGLE_API_KEY! Vui l√≤ng ki·ªÉm tra file .env.")
        st.stop()

    st.set_page_config(
        page_title="Chat v·ªõi PDF (OCR + Table)", 
        page_icon="üìö",
        layout="wide"
    )
    st.write(css, unsafe_allow_html=True)

    # Kh·ªüi t·∫°o session state
    if "conversation" not in st.session_state:
        st.session_state.conversation = None
    if "chat_history_dict" not in st.session_state:
        st.session_state.chat_history_dict = []

    # Header
    st.title("üìö Chat v·ªõi PDF (H·ªó tr·ª£ OCR & B·∫£ng)")
    st.markdown("""
    ### üöÄ T√≠nh nƒÉng:
    - ‚úÖ PDF th∆∞·ªùng (text) + tr√≠ch xu·∫•t b·∫£ng b·∫±ng pdfplumber
    - ‚úÖ PDF scan + OCR b·∫±ng PaddleOCR
    - ‚úÖ T·ª± ƒë·ªông ph√°t hi·ªán lo·∫°i PDF
    - ‚úÖ Chat th√¥ng minh v·ªõi n·ªôi dung PDF
    """)

    # Input c√¢u h·ªèi
    user_question = st.text_input("üí¨ ƒê·∫∑t c√¢u h·ªèi v·ªÅ t√†i li·ªáu c·ªßa b·∫°n:")

    if user_question:
        if st.session_state.conversation is not None:
            handle_userinput_with_manual_history(user_question)
        else:
            st.warning("‚ö†Ô∏è Vui l√≤ng upload v√† x·ª≠ l√Ω PDF tr∆∞·ªõc!")

    # Sidebar
    with st.sidebar:
        st.subheader("üìÅ T·∫£i l√™n t√†i li·ªáu")
        
        pdf_docs = st.file_uploader(
            "Ch·ªçn file PDF (h·ªó tr·ª£ c·∫£ PDF text v√† scan):",
            accept_multiple_files=True,
            type=['pdf']
        )
        
        if pdf_docs:
            st.success(f"‚úÖ ƒê√£ ch·ªçn {len(pdf_docs)} file PDF")
            total_size = sum([pdf.size for pdf in pdf_docs]) / (1024*1024)
            st.info(f"üíæ T·ªïng dung l∆∞·ª£ng: {total_size:.2f} MB")
        
        if st.button("üîÑ X·ª≠ l√Ω PDF", type="primary"):
            if pdf_docs:
                with st.spinner("üîÑ ƒêang x·ª≠ l√Ω PDF..."):
                    # Tr√≠ch xu·∫•t text
                    raw_text = get_pdf_text_enhanced(pdf_docs)
                    
                    if raw_text.strip():
                        # T·∫°o chunks
                        text_chunks = get_text_chunks(raw_text)
                        
                        if text_chunks:
                            # T·∫°o vectorstore
                            vectorstore = get_vectorstore(text_chunks, api_key)
                            
                            if vectorstore:
                                # T·∫°o conversation chain
                                st.session_state.conversation = get_conversation_chain(
                                    vectorstore, api_key
                                )
                                
                                if st.session_state.conversation:
                                    # Reset l·ªãch s·ª≠ chat
                                    st.session_state.chat_history_dict = []
                                    st.success("üéâ X·ª≠ l√Ω ho√†n t·∫•t! B·∫°n c√≥ th·ªÉ ƒë·∫∑t c√¢u h·ªèi ngay.")
                                else:
                                    st.error("‚ùå Kh√¥ng th·ªÉ t·∫°o conversation chain!")
                            else:
                                st.error("‚ùå Kh√¥ng th·ªÉ t·∫°o vector store!")
                        else:
                            st.error("‚ùå Kh√¥ng th·ªÉ t·∫°o text chunks!")
                    else:
                        st.error("‚ùå Kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c text t·ª´ PDF!")
            else:
                st.warning("‚ö†Ô∏è Vui l√≤ng ch·ªçn √≠t nh·∫•t 1 file PDF!")
        
        # N√∫t x√≥a l·ªãch s·ª≠
        if st.button("üóëÔ∏è X√≥a l·ªãch s·ª≠ chat"):
            st.session_state.chat_history_dict = []
            st.success("‚úÖ ƒê√£ x√≥a l·ªãch s·ª≠ chat!")
            st.rerun()

if __name__ == '__main__':
    main()
